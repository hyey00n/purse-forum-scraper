name: Purse Forum Scraper

on:
  workflow_dispatch:
    inputs:
      keywords:
        description: 'ê°€ê²© í‚¤ì›Œë“œ (ì‰¼í‘œë¡œ êµ¬ë¶„: price,cost,paid,spent,$,â‚©,won,krw)'
        required: true
        default: 'price,cost,paid,spent,total,$,â‚©,won,krw,usd,dollar'
      start_page:
        description: 'ì‹œì‘ í˜ì´ì§€'
        required: false
        default: '1'
      max_pages:
        description: 'ìµœëŒ€ í˜ì´ì§€ ìˆ˜'
        required: false
        default: '3'
  
  schedule:
    - cron: '0 0 * * *'
  
  push:
    branches: [ main ]

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: ğŸ“¥ ì½”ë“œ ì²´í¬ì•„ì›ƒ
      uses: actions/checkout@v3
    
    - name: ğŸ Python ì„¤ì •
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: ğŸŒ Chrome ì„¤ì¹˜
      uses: browser-actions/setup-chrome@latest
      with:
        chrome-version: stable
    
    - name: ğŸš— ChromeDriver ì„¤ì¹˜
      uses: nanasess/setup-chromedriver@master
    
    - name: ğŸ“¦ íŒ¨í‚¤ì§€ ì„¤ì¹˜
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: ğŸ”‘ êµ¬ê¸€ ì¸ì¦ ì„¤ì •
      run: |
        echo '${{ secrets.GOOGLE_CREDENTIALS }}' > credentials.json
    
    - name: âš™ï¸ ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸
      run: |
        python -c "
        import re
        
        with open('config.py', 'r', encoding='utf-8') as f:
            content = f.read()
        
        content = content.replace('YOUR_SPREADSHEET_ID_HERE', '${{ secrets.SPREADSHEET_ID }}')
        
        if '${{ github.event_name }}' == 'workflow_dispatch':
            keywords = '${{ github.event.inputs.keywords }}'.replace('\"', '\\\"')
            start_page = '${{ github.event.inputs.start_page }}'
            max_pages = '${{ github.event.inputs.max_pages }}'
            
            content = re.sub(r'PRICE_KEYWORDS = \".*?\"', f'PRICE_KEYWORDS = \"{keywords}\"', content)
            content = re.sub(r'START_PAGE = \d+', f'START_PAGE = {start_page}', content)
            content = re.sub(r'MAX_PAGES = \d+', f'MAX_PAGES = {max_pages}', content)
        
        with open('config.py', 'w', encoding='utf-8') as f:
            f.write(content)
        
        print('âœ… config.py ì—…ë°ì´íŠ¸ ì™„ë£Œ!')
        "
    
    - name: ğŸš€ í¬ë¡¤ëŸ¬ ì‹¤í–‰
      run: |
        python scraper.py
    
    - name: ğŸ“‹ ë¡œê·¸ ì—…ë¡œë“œ
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: scraper-logs
        path: |
          *.log
          screenshots/
