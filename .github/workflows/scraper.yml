name: Purse Forum Scraper

on:
  workflow_dispatch:
    inputs:
      keyword:
        description: 'ê²€ìƒ‰ í‚¤ì›Œë“œ'
        required: true
        default: 'rhinoplasty'
      max_pages:
        description: 'ìµœëŒ€ í˜ì´ì§€ ìˆ˜'
        required: false
        default: '5'
      max_threads:
        description: 'ìµœëŒ€ ìŠ¤ë ˆë“œ ìˆ˜'
        required: false
        default: '50'
  
  schedule:
    - cron: '0 0 * * *'
  
  push:
    branches: [ main ]

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: ğŸ“¥ ì½”ë“œ ì²´í¬ì•„ì›ƒ
      uses: actions/checkout@v3
    
    - name: ğŸ Python ì„¤ì •
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: ğŸŒ Chrome ì„¤ì¹˜
      uses: browser-actions/setup-chrome@latest
      with:
        chrome-version: stable
    
    - name: ğŸš— ChromeDriver ì„¤ì¹˜
      uses: nanasess/setup-chromedriver@master
    
    - name: ğŸ“¦ íŒ¨í‚¤ì§€ ì„¤ì¹˜
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: ğŸ”‘ êµ¬ê¸€ ì¸ì¦ ì„¤ì •
      run: |
        echo '${{ secrets.GOOGLE_CREDENTIALS }}' > credentials.json
    
    - name: âš™ï¸ ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸
      env:
        SPREADSHEET_ID: ${{ secrets.SPREADSHEET_ID }}
        KEYWORD: ${{ github.event.inputs.keyword }}
        MAX_PAGES: ${{ github.event.inputs.max_pages }}
        MAX_THREADS: ${{ github.event.inputs.max_threads }}
        EVENT_NAME: ${{ github.event_name }}
      run: |
        python << 'EOF'
        import os
        
        # config.py ì½ê¸°
        with open('config.py', 'r', encoding='utf-8') as f:
            content = f.read()
        
        # SPREADSHEET_ID êµì²´
        spreadsheet_id = os.environ.get('SPREADSHEET_ID', '')
        content = content.replace('YOUR_SPREADSHEET_ID_HERE', spreadsheet_id)
        
        # workflow_dispatchì¸ ê²½ìš° ì¶”ê°€ ì„¤ì •
        if os.environ.get('EVENT_NAME') == 'workflow_dispatch':
            keyword = os.environ.get('KEYWORD', 'rhinoplasty')
            max_pages = os.environ.get('MAX_PAGES', '5')
            max_threads = os.environ.get('MAX_THREADS', '50')
            
            # SEARCH_KEYWORD êµì²´
            import re
            content = re.sub(
                r'SEARCH_KEYWORD = ".*?"',
                f'SEARCH_KEYWORD = "{keyword}"',
                content
            )
            content = re.sub(
                r'MAX_PAGES = \d+',
                f'MAX_PAGES = {max_pages}',
                content
            )
            content = re.sub(
                r'MAX_THREADS = \d+',
                f'MAX_THREADS = {max_threads}',
                content
            )
        
        # ì €ì¥
        with open('config.py', 'w', encoding='utf-8') as f:
            f.write(content)
        
        print("âœ… config.py ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
        EOF
    
    - name: ğŸš€ í¬ë¡¤ëŸ¬ ì‹¤í–‰
      run: |
        python scraper.py
    
    - name: ğŸ“‹ ë¡œê·¸ ì—…ë¡œë“œ
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: scraper-logs
        path: |
          *.log
          screenshots/