"""
Purse Forum í¬ë¡¤ëŸ¬
í‚¤ì›Œë“œ ê²€ìƒ‰ â†’ ë³¸ë¬¸ ìˆ˜ì§‘ â†’ êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥
"""

import time
import re
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from config import *

class PurseForumScraper:
    def __init__(self):
        self.setup_driver()
        self.setup_google_sheets()
        self.collected_urls = set()
        self.results = []
        
    def setup_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì„¤ì •"""
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        self.driver = webdriver.Chrome(options=chrome_options)
        self.wait = WebDriverWait(self.driver, 10)
        print("âœ… Chrome ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ")
        
    def setup_google_sheets(self):
        """êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ì„¤ì •"""
        try:
            scope = [
                'https://spreadsheets.google.com/feeds',
                'https://www.googleapis.com/auth/drive'
            ]
            
            creds = ServiceAccountCredentials.from_json_keyfile_name(
                GOOGLE_CREDENTIALS_FILE, 
                scope
            )
            
            self.gc = gspread.authorize(creds)
            self.sheet = self.gc.open_by_key(SPREADSHEET_ID).worksheet(SHEET_NAME)
            
            # í—¤ë” ì„¤ì •
            headers = ['ì œëª©', 'URL', 'ì‘ì„±ì', 'ì‘ì„±ì¼', 'ë³¸ë¬¸ ë‚´ìš©', 'ê°€ê²© ì •ë³´', 'ë³‘ì›', 'ìˆ˜ì§‘ì¼ì‹œ']
            
            if not self.sheet.row_values(1):
                self.sheet.update('A1:H1', [headers])
                self.sheet.format('A1:H1', {
                    'backgroundColor': {'red': 0.26, 'green': 0.52, 'blue': 0.96},
                    'textFormat': {'foregroundColor': {'red': 1, 'green': 1, 'blue': 1}, 'bold': True}
                })
            
            print("âœ… êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
    
    def search_forum(self, keyword):
        """í¬ëŸ¼ ê²€ìƒ‰"""
        print(f"\nğŸ” ê²€ìƒ‰ ì‹œì‘: '{keyword}'")
        
        search_url = f"https://forum.purseblog.com/search/search?keywords={keyword}&c[nodes][0]=277&order=relevance"
        
        self.driver.get(search_url)
        time.sleep(3)
        
        print(f"ğŸ“„ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ: {self.driver.current_url}")
        
    def collect_thread_links(self, max_pages=5):
        """ìŠ¤ë ˆë“œ ë§í¬ ìˆ˜ì§‘"""
        print(f"\nğŸ“‹ ë§í¬ ìˆ˜ì§‘ ì¤‘... (ìµœëŒ€ {max_pages}í˜ì´ì§€)")
        
        for page in range(1, max_pages + 1):
            try:
                print(f"\n--- í˜ì´ì§€ {page} ---")
                
                thread_links = self.driver.find_elements(By.CSS_SELECTOR, 'a[href*="/threads/"]')
                
                page_urls = []
                for link in thread_links:
                    try:
                        url = link.get_attribute('href')
                        if url and '/threads/' in url and url not in self.collected_urls:
                            clean_url = url.split('?')[0].split('#')[0]
                            if clean_url not in self.collected_urls:
                                self.collected_urls.add(clean_url)
                                page_urls.append(clean_url)
                    except:
                        continue
                
                print(f"âœ… í˜ì´ì§€ {page}: {len(page_urls)}ê°œ ìƒˆ ë§í¬ ë°œê²¬")
                
                if page < max_pages:
                    try:
                        next_button = self.driver.find_element(By.CSS_SELECTOR, 'a.pageNav-jump--next')
                        next_button.click()
                        time.sleep(2)
                    except NoSuchElementException:
                        print("âš ï¸ ë‹¤ìŒ í˜ì´ì§€ ì—†ìŒ")
                        break
                        
            except Exception as e:
                print(f"âŒ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                break
        
        print(f"\nâœ… ì´ {len(self.collected_urls)}ê°œ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ")
        
    def extract_thread_content(self, url):
        """ê°œë³„ ìŠ¤ë ˆë“œ ë³¸ë¬¸ ì¶”ì¶œ"""
        try:
            self.driver.get(url)
            time.sleep(2)
            
            # ì œëª©
            try:
                title = self.driver.find_element(By.CSS_SELECTOR, 'h1.p-title-value').text
            except:
                title = "No title"
            
            # ì‘ì„±ì
            try:
                author = self.driver.find_element(By.CSS_SELECTOR, 'a.username').text
            except:
                author = "Unknown"
            
            # ì‘ì„±ì¼
            try:
                date = self.driver.find_element(By.CSS_SELECTOR, 'time').get_attribute('datetime')
            except:
                date = ""
            
            # ë³¸ë¬¸ ë‚´ìš©
            try:
                content_div = self.driver.find_element(By.CSS_SELECTOR, 'div.bbWrapper')
                content = content_div.text
                
                content = re.sub(r'\n{3,}', '\n\n', content)
                content = content.strip()
                
                if len(content) > 45000:
                    content = content[:45000] + "\n\n... (ë³¸ë¬¸ ë„ˆë¬´ ê¸¸ì–´ ì¼ë¶€ë§Œ í‘œì‹œ)"
                    
            except:
                content = "No content"
            
            # ê°€ê²© ì •ë³´ ì¶”ì¶œ
            prices = self.extract_prices(title + " " + content)
            price_info = ", ".join(prices) if prices else "No price"
            
            # ë³‘ì› ì •ë³´ ì¶”ì¶œ
            hospitals = self.extract_hospitals(title + " " + content)
            hospital_info = ", ".join(hospitals) if hospitals else "No hospital"
            
            return {
                'title': title,
                'url': url,
                'author': author,
                'date': date,
                'content': content,
                'price': price_info,
                'hospital': hospital_info
            }
            
        except Exception as e:
            print(f"âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
            return None
    
    def extract_prices(self, text):
        """ê°€ê²© ì •ë³´ ì¶”ì¶œ"""
        prices = set()
        
        patterns = [
            r'\$[\d,]+(?:\.\d{2})?',
            r'[\d,]+\s*(?:usd|USD|dollars?)',
            r'â‚©[\d,]+',
            r'[\d,]+\s*(?:won|KRW)',
            r'\$?[\d]+\.?\d*k',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            prices.update(matches)
        
        return list(prices)[:10]
    
    def extract_hospitals(self, text):
        """ë³‘ì› ì´ë¦„ ì¶”ì¶œ"""
        text_lower = text.lower()
        found_hospitals = []
        
        for hospital in HOSPITAL_NAMES:
            if hospital.lower() in text_lower:
                found_hospitals.append(hospital)
        
        return list(set(found_hospitals))[:5]
    
    def save_to_sheet(self):
        """êµ¬ê¸€ ì‹œíŠ¸ì— ì €ì¥"""
        if not self.results:
            print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ’¾ êµ¬ê¸€ ì‹œíŠ¸ì— {len(self.results)}ê°œ ë°ì´í„° ì €ì¥ ì¤‘...")
        
        try:
            existing_rows = len(self.sheet.get_all_values())
            
            now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            rows = []
            
            for result in self.results:
                row = [
                    result['title'],
                    result['url'],
                    result['author'],
                    result['date'],
                    result['content'],
                    result['price'],
                    result['hospital'],
                    now
                ]
                rows.append(row)
            
            if rows:
                start_row = existing_rows + 1
                cell_range = f'A{start_row}:H{start_row + len(rows) - 1}'
                self.sheet.update(cell_range, rows)
                
                print(f"âœ… {len(rows)}ê°œ ë°ì´í„° ì €ì¥ ì™„ë£Œ!")
                
        except Exception as e:
            print(f"âŒ êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def run(self, keyword, max_pages=5, max_threads=50):
        """ë©”ì¸ ì‹¤í–‰"""
        print("=" * 60)
        print("ğŸš€ Purse Forum í¬ë¡¤ëŸ¬ ì‹œì‘")
        print("=" * 60)
        
        try:
            self.search_forum(keyword)
            self.collect_thread_links(max_pages)
            
            print(f"\nğŸ“– ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘... (ìµœëŒ€ {max_threads}ê°œ)")
            
            urls_to_process = list(self.collected_urls)[:max_threads]
            
            for i, url in enumerate(urls_to_process, 1):
                print(f"\n[{i}/{len(urls_to_process)}] {url}")
                
                result = self.extract_thread_content(url)
                
                if result:
                    self.results.append(result)
                    print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {result['title'][:50]}...")
                
                if i < len(urls_to_process):
                    time.sleep(DELAY_BETWEEN_REQUESTS)
            
            self.save_to_sheet()
            
            print("\n" + "=" * 60)
            print("âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
            print(f"ğŸ“Š ì´ ìˆ˜ì§‘: {len(self.results)}ê°œ")
            print("=" * 60)
            
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            
        finally:
            self.driver.quit()
            print("ğŸ”’ ë¸Œë¼ìš°ì € ì¢…ë£Œ")

if __name__ == "__main__":
    scraper = PurseForumScraper()
    scraper.run(
        keyword=SEARCH_KEYWORD,
        max_pages=MAX_PAGES,
        max_threads=MAX_THREADS
    )