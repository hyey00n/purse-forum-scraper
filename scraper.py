"""
Purse Forum í¬ë¡¤ëŸ¬
Asian Plastic Surgery í¬ëŸ¼ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ â†’ êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥
"""

import sys
import time
import re
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from config import *

# ì¦‰ì‹œ ì¶œë ¥ ì„¤ì •
def log(message):
    """ì¦‰ì‹œ ì¶œë ¥ë˜ëŠ” ë¡œê·¸"""
    print(message, flush=True)

class PurseForumScraper:
    def __init__(self):
        log("ğŸ”§ ì´ˆê¸°í™” ì‹œì‘...")
        self.collected_urls = set()
        self.results = []
        
        try:
            self.setup_driver()
            self.setup_google_sheets()
            log("âœ… ì´ˆê¸°í™” ì™„ë£Œ!")
        except Exception as e:
            log(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def setup_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì„¤ì •"""
        log("ğŸŒ Chrome ë“œë¼ì´ë²„ ì„¤ì • ì¤‘...")
        
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        self.driver = webdriver.Chrome(options=chrome_options)
        
        # íƒ€ì„ì•„ì›ƒ ì„¤ì •
        self.driver.set_page_load_timeout(30)
        self.driver.implicitly_wait(10)
        
        self.wait = WebDriverWait(self.driver, 10)
        log("âœ… Chrome ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ (íƒ€ì„ì•„ì›ƒ: 30ì´ˆ)")
    
    def setup_google_sheets(self):
        """êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ì„¤ì •"""
        log("ğŸ“Š êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ì¤‘...")
        
        try:
            scope = [
                'https://spreadsheets.google.com/feeds',
                'https://www.googleapis.com/auth/drive'
            ]
            
            creds = ServiceAccountCredentials.from_json_keyfile_name(
                GOOGLE_CREDENTIALS_FILE, 
                scope
            )
            
            log(f"ğŸ“‹ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ID: {SPREADSHEET_ID}")
            
            self.gc = gspread.authorize(creds)
            self.sheet = self.gc.open_by_key(SPREADSHEET_ID).worksheet(SHEET_NAME)
            
            # í—¤ë” ì„¤ì •
            headers = ['ì œëª©', 'URL', 'ì‘ì„±ì', 'ì‘ì„±ì¼', 'ë³¸ë¬¸ ë‚´ìš©', 'ê°€ê²© ì •ë³´', 'ë³‘ì›', 'ìˆ˜ì§‘ì¼ì‹œ']
            
            if not self.sheet.row_values(1):
                self.sheet.update('A1:H1', [headers])
                self.sheet.format('A1:H1', {
                    'backgroundColor': {'red': 0.26, 'green': 0.52, 'blue': 0.96},
                    'textFormat': {'foregroundColor': {'red': 1, 'green': 1, 'blue': 1}, 'bold': True}
                })
            
            log("âœ… êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ì™„ë£Œ")
            
        except Exception as e:
            log(f"âŒ êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
    
    def search_forum(self, keyword):
        """í¬ëŸ¼ ì„¹ì…˜ ì ‘ì†"""
        log(f"\nğŸ” Asian Plastic Surgery í¬ëŸ¼ ì ‘ì† ì¤‘...")
        
        forum_url = "https://forum.purseblog.com/forums/asian-plastic-surgery-cosmetic-procedures.277/"
        
        try:
            log(f"ğŸŒ URL: {forum_url}")
            log("â³ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘... (ìµœëŒ€ 30ì´ˆ)")
            
            self.driver.get(forum_url)
            
            log("â° 5ì´ˆ ëŒ€ê¸°...")
            time.sleep(5)
            
            log(f"âœ… í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ!")
            log(f"ğŸ“ í˜„ì¬ URL: {self.driver.current_url}")
            log(f"ğŸ“„ í˜ì´ì§€ ì œëª©: {self.driver.title}")
            
        except TimeoutException:
            log(f"âŒ íƒ€ì„ì•„ì›ƒ: í˜ì´ì§€ ë¡œë“œê°€ 30ì´ˆ ì´ˆê³¼")
            log("ğŸ”§ í¬ëŸ¼ ì‚¬ì´íŠ¸ê°€ ëŠë¦¬ê±°ë‚˜ ë´‡ì„ ì°¨ë‹¨í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            raise
        except Exception as e:
            log(f"âŒ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def collect_thread_links(self, max_pages=5, start_page=1):
        """ìŠ¤ë ˆë“œ ë§í¬ ìˆ˜ì§‘ (ëª¨ë“  ìŠ¤ë ˆë“œ)"""
        log(f"\nğŸ“‹ ë§í¬ ìˆ˜ì§‘ ì¤‘... ({start_page}í˜ì´ì§€ë¶€í„° {max_pages}í˜ì´ì§€ê¹Œì§€)")
        
        # ì‹œì‘ í˜ì´ì§€ë¡œ ì´ë™ (1í˜ì´ì§€ê°€ ì•„ë‹Œ ê²½ìš°)
        if start_page > 1:
            log(f"â¡ï¸ {start_page}í˜ì´ì§€ë¡œ ê±´ë„ˆë›°ëŠ” ì¤‘...")
            for skip in range(1, start_page):
                try:
                    next_button = self.driver.find_element(By.CSS_SELECTOR, 'a.pageNav-jump--next')
                    next_button.click()
                    time.sleep(2)
                    log(f"âœ… {skip + 1}í˜ì´ì§€ë¡œ ì´ë™")
                except NoSuchElementException:
                    log(f"âš ï¸ {skip}í˜ì´ì§€ì—ì„œ ë‹¤ìŒ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    break
                except Exception as e:
                    log(f"âŒ í˜ì´ì§€ ê±´ë„ˆë›°ê¸° ì‹¤íŒ¨: {e}")
                    break
        
        for page in range(start_page, max_pages + 1):
            try:
                log(f"\n--- í˜ì´ì§€ {page} ---")
                
                thread_links = self.driver.find_elements(By.CSS_SELECTOR, 'a[href*="/threads/"]')
                log(f"ğŸ”— ë°œê²¬ëœ ë§í¬ ìˆ˜: {len(thread_links)}")
                
                page_urls = []
                for link in thread_links:
                    try:
                        url = link.get_attribute('href')
                        if url and '/threads/' in url:
                            clean_url = url.split('?')[0].split('#')[0]
                            if clean_url not in self.collected_urls:
                                self.collected_urls.add(clean_url)
                                page_urls.append(clean_url)
                    except:
                        continue
                
                log(f"âœ… í˜ì´ì§€ {page}: {len(page_urls)}ê°œ ìƒˆ ë§í¬ ë°œê²¬")
                
                # ë‹¤ìŒ í˜ì´ì§€
                if page < max_pages:
                    try:
                        next_button = self.driver.find_element(By.CSS_SELECTOR, 'a.pageNav-jump--next')
                        log("â¡ï¸ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™...")
                        next_button.click()
                        time.sleep(2)
                    except NoSuchElementException:
                        log("âš ï¸ ë‹¤ìŒ í˜ì´ì§€ ì—†ìŒ (ë§ˆì§€ë§‰ í˜ì´ì§€)")
                        break
                        
            except Exception as e:
                log(f"âŒ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                break
        
        log(f"\nâœ… ì´ {len(self.collected_urls)}ê°œ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ")
    
    def extract_thread_content(self, url):
        """ê°œë³„ ìŠ¤ë ˆë“œ ë³¸ë¬¸ ì¶”ì¶œ"""
        try:
            self.driver.get(url)
            time.sleep(2)
            
            # ì œëª©
            try:
                title = self.driver.find_element(By.CSS_SELECTOR, 'h1.p-title-value').text
            except:
                title = "No title"
            
            # ì‘ì„±ì
            try:
                author = self.driver.find_element(By.CSS_SELECTOR, 'a.username').text
            except:
                author = "Unknown"
            
            # ì‘ì„±ì¼
            try:
                date = self.driver.find_element(By.CSS_SELECTOR, 'time').get_attribute('datetime')
            except:
                date = ""
            
            # ë³¸ë¬¸ ë‚´ìš©
            try:
                content_div = self.driver.find_element(By.CSS_SELECTOR, 'div.bbWrapper')
                content = content_div.text
                
                content = re.sub(r'\n{3,}', '\n\n', content)
                content = content.strip()
                
                if len(content) > 45000:
                    content = content[:45000] + "\n\n... (ë³¸ë¬¸ ë„ˆë¬´ ê¸¸ì–´ ì¼ë¶€ë§Œ í‘œì‹œ)"
                    
            except:
                content = "No content"
            
            # ê°€ê²© ì •ë³´ ì¶”ì¶œ
            prices = self.extract_prices(title + " " + content)
            price_info = ", ".join(prices) if prices else "No price"
            
            # ë³‘ì› ì •ë³´ ì¶”ì¶œ
            hospitals = self.extract_hospitals(title + " " + content)
            hospital_info = ", ".join(hospitals) if hospitals else "No hospital"
            
            return {
                'title': title,
                'url': url,
                'author': author,
                'date': date,
                'content': content,
                'price': price_info,
                'hospital': hospital_info
            }
            
        except Exception as e:
            log(f"âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
            return None
    
    def extract_prices(self, text):
        """ê°€ê²© ì •ë³´ ì¶”ì¶œ"""
        prices = set()
        
        patterns = [
            r'\$[\d,]+(?:\.\d{2})?',
            r'[\d,]+\s*(?:usd|USD|dollars?)',
            r'â‚©[\d,]+',
            r'[\d,]+\s*(?:won|KRW)',
            r'\$?[\d]+\.?\d*k',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            prices.update(matches)
        
        return list(prices)[:10]
    
    def extract_hospitals(self, text):
        """ë³‘ì› ì´ë¦„ ì¶”ì¶œ"""
        text_lower = text.lower()
        found_hospitals = []
        
        for hospital in HOSPITAL_NAMES:
            if hospital.lower() in text_lower:
                found_hospitals.append(hospital)
        
        return list(set(found_hospitals))[:5]
    
    def save_to_sheet(self):
        """êµ¬ê¸€ ì‹œíŠ¸ì— ì €ì¥ (ê°€ê²© ì •ë³´ ìˆëŠ” ê²ƒë§Œ)"""
        if not self.results:
            log("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ê°€ê²© ê´€ë ¨ í‚¤ì›Œë“œ
        price_keywords = ['price', 'cost', 'paid', 'spent', 'total', 'usd', 'krw', 'won', 'dollar', '$', 'â‚©']
        
        # ê°€ê²© ì •ë³´ê°€ ìˆëŠ” ê²Œì‹œê¸€ë§Œ í•„í„°ë§
        filtered_results = []
        for result in self.results:
            text = (result['title'] + " " + result['content']).lower()
            
            # ê°€ê²© ì •ë³´ê°€ ìˆê±°ë‚˜ ê°€ê²© í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ í¬í•¨
            has_price = result['price'] != "No price"
            has_keyword = any(keyword in text for keyword in price_keywords)
            
            if has_price or has_keyword:
                filtered_results.append(result)
                log(f"âœ… ê°€ê²© ì •ë³´ ë°œê²¬: {result['title'][:50]}")
            else:
                log(f"â­ï¸ ê°€ê²© ì—†ìŒ ê±´ë„ˆëœ€: {result['title'][:50]}")
        
        if not filtered_results:
            log("âš ï¸ ê°€ê²© ì •ë³´ê°€ ìˆëŠ” ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        log(f"\nğŸ’¾ êµ¬ê¸€ ì‹œíŠ¸ì— {len(filtered_results)}ê°œ ë°ì´í„° ì €ì¥ ì¤‘... (ì „ì²´ {len(self.results)}ê°œ ì¤‘)")
        
        try:
            existing_rows = len(self.sheet.get_all_values())
            
            now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            rows = []
            
            for result in filtered_results:
                row = [
                    result['title'],
                    result['url'],
                    result['author'],
                    result['date'],
                    result['content'],
                    result['price'],
                    result['hospital'],
                    now
                ]
                rows.append(row)
            
            if rows:
                start_row = existing_rows + 1
                cell_range = f'A{start_row}:H{start_row + len(rows) - 1}'
                self.sheet.update(cell_range, rows)
                
                log(f"âœ… {len(rows)}ê°œ ë°ì´í„° ì €ì¥ ì™„ë£Œ!")
                log(f"ğŸ“Š êµ¬ê¸€ ì‹œíŠ¸: https://docs.google.com/spreadsheets/d/{SPREADSHEET_ID}")
                
        except Exception as e:
            log(f"âŒ êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            
    def run(self, keyword, max_pages=5, max_threads=50, start_page=1):
        """ë©”ì¸ ì‹¤í–‰"""
        log("=" * 60)
        log("ğŸš€ Purse Forum í¬ë¡¤ëŸ¬ ì‹œì‘")
        log("=" * 60)
        
        try:
            # 1. í¬ëŸ¼ ì ‘ì†
            self.search_forum(keyword)
            
            # 2. ë§í¬ ìˆ˜ì§‘ (start_pageë¶€í„° ì‹œì‘)
            self.collect_thread_links(max_pages, start_page)
            
            if len(self.collected_urls) == 0:
                log("âš ï¸ ìˆ˜ì§‘ëœ ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤!")
                return
            
            # 3. ë³¸ë¬¸ ìˆ˜ì§‘
            log(f"\nğŸ“– ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘... (ìµœëŒ€ {max_threads}ê°œ)")
            
            urls_to_process = list(self.collected_urls)[:max_threads]
            
            for i, url in enumerate(urls_to_process, 1):
                log(f"\n[{i}/{len(urls_to_process)}] {url}")
                
                result = self.extract_thread_content(url)
                
                if result:
                    self.results.append(result)
                    log(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {result['title'][:50]}...")
                
                if i < len(urls_to_process):
                    time.sleep(DELAY_BETWEEN_REQUESTS)
            
            # 4. êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥
            self.save_to_sheet()
            
            log("\n" + "=" * 60)
            log("âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
            log(f"ğŸ“Š ì´ ìˆ˜ì§‘: {len(self.results)}ê°œ")
            log("=" * 60)
            
        except Exception as e:
            log(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            
        finally:
            try:
                self.driver.quit()
                log("ğŸ”’ ë¸Œë¼ìš°ì € ì¢…ë£Œ")
            except:
                pass

if __name__ == "__main__":
    log("=" * 60)
    log("í”„ë¡œê·¸ë¨ ì‹œì‘!")
    log("=" * 60)
    
    try:
        scraper = PurseForumScraper()
        scraper.run(
            keyword=SEARCH_KEYWORD,
            max_pages=MAX_PAGES,
            max_threads=MAX_THREADS,
            start_page=START_PAGE
        )
    except Exception as e:
        log(f"âŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)